{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "265de128",
      "metadata": {
        "id": "265de128"
      },
      "source": [
        "<strong>\n",
        "    <font color=\"#0E1117\">\n",
        "        Author: lprtk\n",
        "    </font>\n",
        "</strong>\n",
        "\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "\n",
        "<Center>\n",
        "    <h1 style=\"font-family: Arial\">\n",
        "        <font color=\"#0E1117\">\n",
        "            NLP: sentiment analysis, topic modeling & sentiment prediction\n",
        "        </font>\n",
        "    </h1>\n",
        "    <h3 style=\"font-family: Arial\">\n",
        "        <font color=\"#0E1117\">\n",
        "            Notebook 1/5\n",
        "        </font>\n",
        "    </h3>\n",
        "</Center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9faf7e04",
      "metadata": {
        "id": "9faf7e04"
      },
      "source": [
        "-------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "526f6c58",
      "metadata": {
        "id": "526f6c58"
      },
      "source": [
        "<div style=\"margin: 10px;\">\n",
        "    <h2 style=\"font-family: Arial\">\n",
        "        <font color=\"#0E1117\">\n",
        "            Introduction & context\n",
        "        </font>\n",
        "    </h2>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3158c1a5",
      "metadata": {
        "id": "3158c1a5"
      },
      "source": [
        "<p style=\"text-align: justify\">\n",
        "    This project focuses on extracting information and value from large volumes of textual data using Natural Language Processing (NLP). Why do you want to do this?\n",
        "</p>\n",
        "<ul>\n",
        "    <li><p style=\"text-align: justify\">To improve the customer experience on the website, mobile application or in the office.</p></li>\n",
        "    <li><p style=\"text-align: justify\">Assess customer satisfaction differently.</p></li>\n",
        "    <li><p style=\"text-align: justify\"></p>Evaluate the company's image.</li>\n",
        "    <li><p style=\"text-align: justify\"></p>Be more available and accessible to customers.</li>\n",
        "    <li><p style=\"text-align: justify\"></p>According to the company's activity: find new solutions to improve the banking services offered, evaluate the seller of an online sales platform or improve the product based on customer reviews.</li>\n",
        "</ul>\n",
        "\n",
        "<p style=\"text-align: justify\">\n",
        "    Our application approach is presented in 5 main streams:\n",
        "</p>\n",
        "<ul>\n",
        "    <li>\n",
        "        <u>Step 1:</u> Web Scraping\n",
        "        <ul>\n",
        "            <li>Collect and create the data schema.</li>\n",
        "            <li>Parsing customer reviews to enrich the database: extracting title, description, date, time, nickname and rating.</li>\n",
        "        </ul>\n",
        "    </li>\n",
        "</ul>\n",
        "<ul>\n",
        "    <li>\n",
        "        <u>Step 2:</u> Sentiment Analysis and Scoring\n",
        "        <ul>\n",
        "            <li>Understand and probe the satisfaction of each customer.</li>\n",
        "            <li>Scoring the intensity and polarity of feelings from the review description.</li>\n",
        "        </ul>\n",
        "    </li>\n",
        "</ul>\n",
        "<ul>\n",
        "    <li>\n",
        "        <u>Step 3:</u> Text mining and data cleaning\n",
        "        <ul>\n",
        "            <li>Text cleaning adapted to the sales domain and to the general content of reviews.</li>\n",
        "        </ul>\n",
        "    </li>\n",
        "</ul>\n",
        "<ul>\n",
        "    <li>\n",
        "        <u>Step 4:</u> Topic Modeling (unsupervised learning)\n",
        "        <ul>\n",
        "            <li>To improve availability and speed up response time, reviews can be disassociated and prioritized according to the topic they address.</li>\n",
        "        </ul>\n",
        "    </li>\n",
        "</ul>\n",
        "<ul>\n",
        "    <li>\n",
        "        <u>Step 5:</u> Machine Learning (supervised learning)\n",
        "        <ul>\n",
        "            <li>Without reading future reviews, design a robust model to identify the overall sentiment expressed by the customer.</li>\n",
        "        </ul>\n",
        "    </li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2246153f",
      "metadata": {
        "id": "2246153f"
      },
      "source": [
        "-------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1f37968",
      "metadata": {
        "id": "b1f37968"
      },
      "source": [
        "<div style=\"margin: 10px;\">\n",
        "    <h2 style=\"font-family: Arial\">\n",
        "        <font color=\"#0E1117\">\n",
        "            Librairies import\n",
        "        </font>\n",
        "    </h2>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9319bb21",
      "metadata": {
        "id": "9319bb21"
      },
      "outputs": [],
      "source": [
        "import bs4\n",
        "import pandas as pd\n",
        "import requests\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0904b031",
      "metadata": {
        "id": "0904b031"
      },
      "source": [
        "-------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "758aec8a",
      "metadata": {
        "id": "758aec8a"
      },
      "source": [
        "<div style=\"margin: 10px;\">\n",
        "    <h2 style=\"font-family: Arial\">\n",
        "        <font color=\"#0E1117\">\n",
        "            Web scraping\n",
        "        </font>\n",
        "    </h2>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "299e21f6",
      "metadata": {
        "id": "299e21f6"
      },
      "source": [
        "<div style=\"margin: 10px;\">\n",
        "    <h3 style=\"font-family: Arial\">\n",
        "        <font color=\"#0E1117\">\n",
        "            1) Function initialization\n",
        "        </font>\n",
        "    </h3>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ede7acd0",
      "metadata": {
        "id": "ede7acd0"
      },
      "outputs": [],
      "source": [
        "def get_data(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Function that allows to do a web request to a web page and access to\n",
        "    his content.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    url : str\n",
        "        Internet address: uniform resource locator (url).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        HTML body of a web page.\n",
        "\n",
        "    \"\"\"\n",
        "    content = requests.get(url, headers=headers)\n",
        "\n",
        "    return content.text\n",
        "\n",
        "\n",
        "def html_code(url: str) -> bs4.BeautifulSoup:\n",
        "    \"\"\"\n",
        "    Function that allows to parse the HTML body of a web page.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    url : str\n",
        "        Internet address: uniform resource locator (url).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bs4.BeautifulSoup\n",
        "        Parsed HTML body of a web page.\n",
        "\n",
        "    \"\"\"\n",
        "    html_data = get_data(url=url)\n",
        "    soup = bs4.BeautifulSoup(html_data, \"html.parser\")\n",
        "\n",
        "    return soup\n",
        "\n",
        "\n",
        "def cus_data(soup: str, html_tag: str, html_class: str) -> list:\n",
        "    \"\"\"\n",
        "    Function that allows to extract text from an HTML file parsed according\n",
        "    to a precise location.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    soup : bs4.BeautifulSoup\n",
        "        Parsed HTML body of a web page.\n",
        "\n",
        "    html_tag : str\n",
        "        HTML tag which indicates what to scrape.\n",
        "\n",
        "    html_class : str\n",
        "        HTML class which indicates what to scrape.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        Scraped text.\n",
        "\n",
        "    \"\"\"\n",
        "    cus_list = []\n",
        "    for item in soup.find_all(html_tag, class_=html_class):\n",
        "        data_str = item.get_text()\n",
        "        cus_list.append(data_str)\n",
        "\n",
        "    return cus_list\n",
        "def cus_verified(soup: str, html_tag: str, html_class: str) -> list:\n",
        "    \"\"\"\n",
        "    Function that allows to extract text from an HTML file parsed according\n",
        "    to a precise location.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    soup : bs4.BeautifulSoup\n",
        "        Parsed HTML body of a web page.\n",
        "\n",
        "    html_tag : str\n",
        "        HTML tag which indicates what to scrape.\n",
        "\n",
        "    html_class : str\n",
        "        HTML class which indicates what to scrape.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        Scraped text.\n",
        "\n",
        "    \"\"\"\n",
        "    cus_list = []\n",
        "    #reviews are encapsulated in a class with the tag a-section review aok-relative. we need to extract this class first\n",
        "\n",
        "    for item in soup.find_all(class_= \"a-section review aok-relative\"):\n",
        "\n",
        "        #we need to check if the review is verified. if it is, it will contain an item of the class a-size-mini a-color-state a-text-bold. if it is verified, we append True to the list, else we append False\n",
        "        if item.find(html_tag, class_=html_class):\n",
        "            cus_list.append(True)\n",
        "        else:\n",
        "            cus_list.append(False)\n",
        "    return cus_list\n",
        "\n",
        "def scraped_data(soup: str) -> pd.core.frame.DataFrame:\n",
        "    \"\"\"\n",
        "    Function that calls a scraping function and formats the scraped data\n",
        "    into a pandas.core.frame.DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    soup : bs4.BeautifulSoup\n",
        "        Parsed HTML body of a web page.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pandas.core.frame.DataFrame\n",
        "        Scraped database.\n",
        "\n",
        "    \"\"\"\n",
        "    pseudo = cus_data(\n",
        "        soup=soup,\n",
        "        html_tag=\"span\",\n",
        "        html_class=\"a-profile-name\"\n",
        "    )\n",
        "    title = cus_data(\n",
        "        soup=soup,\n",
        "        html_tag=\"a\",\n",
        "        html_class=\"a-size-base a-link-normal review-title a-color-base review-title-content a-text-bold\"\n",
        "    )\n",
        "    review = cus_data(\n",
        "        soup=soup,\n",
        "        html_tag=\"span\",\n",
        "        html_class=\"a-size-base review-text review-text-content\"\n",
        "    )\n",
        "    rating = cus_data(\n",
        "        soup=soup,\n",
        "        html_tag=\"span\",\n",
        "        html_class=\"a-icon-alt\"\n",
        "    )\n",
        "    verified_purchase = cus_verified(\n",
        "        soup=soup,\n",
        "        html_tag=\"span\",\n",
        "        html_class=\"a-size-mini a-color-state a-text-bold\"\n",
        "    )\n",
        "    date = cus_data(\n",
        "        soup=soup,\n",
        "        html_tag=\"span\",\n",
        "        html_class=\"a-size-base a-color-secondary review-date\"\n",
        "    )\n",
        "    product = cus_data(\n",
        "        soup=soup,\n",
        "        html_tag=\"div\",\n",
        "        html_class=\"a-row product-title\"\n",
        "    )\n",
        "\n",
        "    df_data = pd.DataFrame(\n",
        "        {\n",
        "            \"Pseudo\": [\"\"],\n",
        "            \"Title\": [\"\"],\n",
        "            \"Review\": [\"\"],\n",
        "            \"Rating\": [\"\"],\n",
        "            \"Verified Purchase\": [\"\"],\n",
        "            \"Date\": [\"\"]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    for i in range(len(title)):\n",
        "        df_scraped = pd.DataFrame(\n",
        "            {\n",
        "                \"Pseudo\": [pseudo[i]],\n",
        "                \"Title\": [title[i]],\n",
        "                \"Review\": [review[i]],\n",
        "                \"Rating\": [rating[i]],\n",
        "                \"Verified Purchase\": [verified_purchase[i]],\n",
        "                \"Date\": [date[i]]\n",
        "            }\n",
        "        )\n",
        "\n",
        "        df_data = pd.concat([df_data, df_scraped], ignore_index=True)\n",
        "    df_data.drop([0], axis=0, inplace=True)\n",
        "    return df_data\n",
        "\n",
        "\n",
        "def scraped_data_multipage(url: str) -> pd.core.frame.DataFrame:\n",
        "    \"\"\"\n",
        "    Function that allows you to scrape across multiple review pages and\n",
        "    formats the scraped datainto a pandas.core.frame.DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    url : str\n",
        "        Internet address: uniform resource locator (url).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pandas.core.frame.DataFrame\n",
        "        Scraped database.\n",
        "\n",
        "    \"\"\"\n",
        "    page1 = \"&pageNumber=1\"\n",
        "    page2 = \"&pageNumber=2\"\n",
        "    page3 = \"&pageNumber=3\"\n",
        "    page4 = \"&pageNumber=4\"\n",
        "    page5 = \"&pageNumber=5\"\n",
        "\n",
        "    soup = html_code(url + page1)\n",
        "    data_page1 = scraped_data(soup=soup)\n",
        "\n",
        "    soup = html_code(url + page2)\n",
        "    data_page2 = scraped_data(soup=soup)\n",
        "\n",
        "    soup = html_code(url + page3)\n",
        "    data_page3 = scraped_data(soup=soup)\n",
        "\n",
        "    soup = html_code(url + page4)\n",
        "    data_page4 = scraped_data(soup=soup)\n",
        "\n",
        "    soup = html_code(url + page5)\n",
        "    data_page5 = scraped_data(soup=soup)\n",
        "\n",
        "    df_data = pd.concat(\n",
        "        [\n",
        "            data_page1,\n",
        "            data_page2,\n",
        "            data_page3,\n",
        "            data_page4,\n",
        "            data_page5\n",
        "        ]\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    return df_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d716cc51",
      "metadata": {
        "id": "d716cc51"
      },
      "outputs": [],
      "source": [
        "headers = (\n",
        "    {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\\\n",
        "        AppleWebKit/537.36 (KHTML, like Gecko)\\\n",
        "        Chrome/90.0.4430.212 Safari/537.36\",\n",
        "        \"Accept-Language\": \"en-US, en;q=0.5\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "870d0da0",
      "metadata": {
        "id": "870d0da0"
      },
      "source": [
        "<div style=\"margin: 10px;\">\n",
        "    <h3 style=\"font-family: Arial\">\n",
        "        <font color=\"#0E1117\">\n",
        "            2) Function application\n",
        "        </font>\n",
        "    </h3>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e4ea167",
      "metadata": {
        "id": "1e4ea167"
      },
      "outputs": [],
      "source": [
        "# list of urls to scrape\n",
        "list_urls = [\n",
        "    \"https://www.amazon.com/Razer-Blade-14-Gaming-Laptop/product-reviews/B094658SMY/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n",
        "    \"https://www.amazon.com/ASUS-Processor-NumberPad-Microsoft-L210MA-DB01/product-reviews/B081V6W99V/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n",
        "    \"https://www.amazon.com/HP-Generation-i5-1135G7-Graphics-15-dy2024nr/product-reviews/B09FXFDGN3/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n",
        "    \"https://www.amazon.com/ASUS-IPS-Type-i5-10300H-Keyboard-FX706LI-ES53/product-reviews/B08ZLC661T/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n",
        "    \"https://www.amazon.com/Acer-i7-1165G7-Graphics-Antimicrobial-SF514-55TA-74EC/product-reviews/B08JQKMFFB/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n",
        "    \"https://www.amazon.com/HP-Portable-Micro-Edge-Anti-Glare-14-fq1025nr/product-reviews/B09G8SK2KK/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n",
        "    \"https://www.amazon.com/ASUS-VivoBook-R7-3700U-Fingerprint-F512DA-NH77/product-reviews/B085344M9Q/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n",
        "    \"https://www.amazon.com/Dell-Inspiron-3000-Laptop-Celeron/product-reviews/B09F626YKW/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n",
        "    \"https://www.amazon.com/Acer-AN515-55-53E5-i5-10300H-GeForce-Keyboard/product-reviews/B092YHJGMN/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n",
        "    \"https://www.amazon.com/Lenovo-Chromebook-11-6-Inch-Processor-82HG0006US/product-reviews/B08T6N424Z/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n",
        "    \"https://www.amazon.com/Apple-32GB-Space-Model-Refurbished/product-reviews/B074PWW6NS/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n",
        "    \"https://www.amazon.com/ASUS-Chromebook-Spill-resistant-Transparent-CX1100CNA-AS42/product-reviews/B08XTB1NNH/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n",
        "    \"https://www.amazon.com/Acer-A515-56-36UT-Display-i3-1115G4-Processor/product-reviews/B08VKT45K4/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n",
        "    \"https://www.amazon.com/ASUS-Processor-NumberPad-Microsoft-L210MA-DB01/product-reviews/B081V6W99V/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n",
        "    \"https://www.amazon.com/HP-Pavilion-Generation-i7-1165G7-15-eg0025nr/product-reviews/B09FX1YF28/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n",
        "    \"https://www.amazon.com/Samsung-5100mAh-Battery-SM-T290-International/product-reviews/B07XJZ7VQD/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n",
        "    \"https://www.amazon.com/HP-14-Laptop-Dual-Core-Processor/product-reviews/B09VRX9YVW/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n",
        "    \"https://www.amazon.com/MSI-Stealth-15M-Gaming-Laptop/product-reviews/B091GGZT1S/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n",
        "    \"https://www.amazon.com/Apple-MacBook-Retina-MPTR2LL-Renewed/product-reviews/B07JMLMVKP/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n",
        "    \"https://www.amazon.com/Lenovo-Processor-Graphics-82HU00JWUS-Graphite/product-reviews/B09BG96KFJ/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n",
        "    \"https://www.amazon.com/Samsung-Chromebook-XE500C13-K04US-Certified-Refurbished/product-reviews/B0759YSF4W/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n",
        "    \"https://www.amazon.com/Samsung-Chromebook-Celeron-Processor-Gigabit/product-reviews/B07XQQTVS3/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n",
        "    \"https://www.amazon.com/ASUS-Chromebook-Touchscreen-Processor-C433TA-AS384T/product-reviews/B08ZLF99VD/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n",
        "    \"https://www.amazon.com/Acer-Chromebook-Celeron-Display-CB314-1H-C884/product-reviews/B0858N8CGX/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_data = pd.DataFrame(\n",
        "    {\n",
        "        \"Pseudo\": [\"\"],\n",
        "        \"Title\": [\"\"],\n",
        "        \"Review\": [\"\"],\n",
        "        \"Rating\": [\"\"],\n",
        "        \"Verified Purchase\": [\"\"],\n",
        "        \"Date\": [\"\"]\n",
        "    }\n",
        ")\n",
        "for i in list_urls:\n",
        "    #concat the scraped data to the dataframe\n",
        "    df_data = pd.concat([df_data, scraped_data_multipage(i)], ignore_index=True)"
      ],
      "metadata": {
        "id": "NcW7j40PpBW7"
      },
      "id": "NcW7j40PpBW7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14a92e0c",
      "metadata": {
        "id": "14a92e0c"
      },
      "outputs": [],
      "source": [
        "df_data.drop([0], axis=0, inplace=True)\n",
        "df_data.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbe2a7a5",
      "metadata": {
        "id": "dbe2a7a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "959d37dc-c5a8-478e-bd6f-91cd8d9d366d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1099 entries, 0 to 1098\n",
            "Data columns (total 6 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   Pseudo             1099 non-null   object\n",
            " 1   Title              1099 non-null   object\n",
            " 2   Review             1099 non-null   object\n",
            " 3   Rating             1099 non-null   object\n",
            " 4   Verified Purchase  1099 non-null   object\n",
            " 5   Date               1099 non-null   object\n",
            "dtypes: object(6)\n",
            "memory usage: 51.6+ KB\n"
          ]
        }
      ],
      "source": [
        "df_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2973808",
      "metadata": {
        "id": "c2973808"
      },
      "source": [
        "-------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.drop_duplicates(subset = [\"Review\", \"Title\", \"Date\"], keep = False, inplace=True)\n",
        "\n",
        "df_data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIfNgcRGrzjb",
        "outputId": "ef643afd-2064-4409-ea31-221f2ce29c26"
      },
      "id": "HIfNgcRGrzjb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 991 entries, 0 to 1098\n",
            "Data columns (total 6 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   Pseudo             991 non-null    object\n",
            " 1   Title              991 non-null    object\n",
            " 2   Review             991 non-null    object\n",
            " 3   Rating             991 non-null    object\n",
            " 4   Verified Purchase  991 non-null    object\n",
            " 5   Date               991 non-null    object\n",
            "dtypes: object(6)\n",
            "memory usage: 54.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd2d1229",
      "metadata": {
        "id": "cd2d1229"
      },
      "source": [
        "<div style=\"margin: 10px;\">\n",
        "    <h2 style=\"font-family: Arial\">\n",
        "        <font color=\"#0E1117\">\n",
        "            Data export\n",
        "        </font>\n",
        "    </h2>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a7df0a1",
      "metadata": {
        "id": "1a7df0a1"
      },
      "outputs": [],
      "source": [
        "df_data.to_csv(path_or_buf=\"amzn_customer_reviews.csv\", sep=\",\", index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}